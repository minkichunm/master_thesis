{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c83f182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data set\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "train_set, test_set = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f207c035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a simple NN model\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "    \n",
    "kernel_initializer = 'he_normal'\n",
    "activation = \"relu\"\n",
    "\n",
    "def get_model(chs=256):\n",
    "    shape=(28,28,1)\n",
    "    \n",
    "    inputs = Input(shape)\n",
    "    layer = Flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49010b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a simple NN model\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "    \n",
    "kernel_initializer = 'he_normal'\n",
    "activation = \"relu\"\n",
    "\n",
    "def get_model(chs=256):\n",
    "    shape=(28,28,1)\n",
    "    \n",
    "    inputs = Input(shape)\n",
    "    layer = Flatten()(inputs)\n",
    "    layer = Dense(units=chs, activation=activation, kernel_initializer=kernel_initializer)(layer)\n",
    "    layer = Dense(units=chs, activation=activation, kernel_initializer=kernel_initializer)(layer)\n",
    "    output = Dense(10, activation='linear', use_bias=True, kernel_initializer=kernel_initializer)(layer)\n",
    "\n",
    "    model = Model(inputs, output)\n",
    "    return model\n",
    "\n",
    "model = get_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a7a1dc7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'dense_9/kernel:0' shape=(784, 256) dtype=float32, numpy=\n",
       "array([[-0.02880584,  0.04084021,  0.05664704, ...,  0.01978791,\n",
       "         0.03381465, -0.00479045],\n",
       "       [-0.03906069, -0.07141214, -0.02805506, ...,  0.0266929 ,\n",
       "         0.04586157,  0.07790492],\n",
       "       [ 0.05221974,  0.00223555, -0.00929282, ..., -0.06812875,\n",
       "        -0.00967157,  0.0981956 ],\n",
       "       ...,\n",
       "       [-0.07523067, -0.04610952,  0.04351881, ..., -0.00605765,\n",
       "        -0.00031668, -0.00746796],\n",
       "       [-0.00727552,  0.07479773,  0.00788088, ...,  0.00595186,\n",
       "         0.0009602 ,  0.05531799],\n",
       "       [ 0.06108871, -0.02102383,  0.04120374, ..., -0.01849575,\n",
       "        -0.07379042, -0.0147734 ]], dtype=float32)>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "991b1111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(variables):\n",
    "    # TODO implement\n",
    "    # here you need to also output the reange of the compressed data (minimum, maximum)\n",
    "    flat_vars = variables.flatten()\n",
    "    hist, bins = np.histogram(flat_vars, bins='auto')\n",
    "    probs = hist / np.sum(hist)\n",
    "    probs[probs == 0] = 1  # replace zeros with ones\n",
    "    entropy = -np.sum(probs * np.log2(probs))\n",
    "    return entropy, (np.min(flat_vars), np.max(flat_vars))\n",
    "\n",
    "class CompressibleNN(keras.Model):\n",
    "    def __init__(self, net_model):\n",
    "        super(CompressibleNN, self).__init__()\n",
    "        self.net_model = net_model\n",
    "        self.CE_loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.net_model(inputs)\n",
    "    \n",
    "    def entropy_loss(self, inputs):\n",
    "        # TODO implemment the entropy calculation here for each layer parameters\n",
    "        # for each layer: calculate a histogram and from then calculate the entropy\n",
    "        # return a sum of all entropies\n",
    "        \n",
    "        entropy = 0\n",
    "        # for l in self.net_model.layers:\n",
    "        #     #TODO: if layer is dense:\n",
    "        #     for v in l.trainable_variables:\n",
    "        #         # here you have a set of variable for which you can calculate the histogram and entropy\n",
    "        #         entropy += calculate_entropy(v)\n",
    "        # return 0\n",
    "        \n",
    "        for l in self.net_model.layers:\n",
    "            if isinstance(l, keras.layers.Dense):\n",
    "                for v in l.trainable_variables:\n",
    "                    v_entropy, v_range = calculate_entropy(v.numpy().flatten())\n",
    "                    entropy += v_entropy\n",
    "            \n",
    "        return entropy\n",
    "\n",
    "    def train_step(self, input):\n",
    "        images = input[0]\n",
    "        labels = input[1]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            output = self.net_model(images)\n",
    "            loss = self.CE_loss(labels, output) + \\\n",
    "                    self.entropy_loss(images)\n",
    "\n",
    "        # Get the gradients w.r.t the loss\n",
    "        gradient = tape.gradient(loss, self.net_model.trainable_variables)\n",
    "        # Update the weights using the generator optimizer\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradient, self.net_model.trainable_variables)\n",
    "        )\n",
    "        return {\"loss\": loss}\n",
    "    \n",
    "compNN = CompressibleNN(model)\n",
    "optimizer = tf.optimizers.Adam(learning_rate=1e-3, beta_1=0.9)\n",
    "compNN.compile(optimizer, loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4fd59b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1501\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b6e64e0730>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "\n",
    "compNN.fit(x=train_set[0], y=train_set[1], epochs=1, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "22f9f5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc:  95.32000000000001\n"
     ]
    }
   ],
   "source": [
    "# test performance\n",
    "res = model(test_set[0])\n",
    "print(\"test acc: \", 100*(np.argmax(res, axis=1)==test_set[1]).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
