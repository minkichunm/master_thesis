{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83f182a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST data set\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "train_set, test_set = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49010b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a simple NN model\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "    \n",
    "kernel_initializer = 'he_normal'\n",
    "activation = \"relu\"\n",
    "\n",
    "def get_model(chs=128):\n",
    "    shape=(28,28,1)\n",
    "    \n",
    "    inputs = Input(shape)\n",
    "    layer = Flatten()(inputs)\n",
    "    layer = Dense(units=chs*2, activation=activation, kernel_initializer=kernel_initializer)(layer)\n",
    "    layer = Dense(units=chs, activation=activation, kernel_initializer=kernel_initializer)(layer)\n",
    "    output = Dense(10, activation='linear', use_bias=True, kernel_initializer=kernel_initializer)(layer)\n",
    "\n",
    "    model = Model(inputs, output)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1c0f01-a616-4d3b-822c-6ebe0f5f262c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dahuffman\n",
      "  Downloading dahuffman-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Installing collected packages: dahuffman\n",
      "Successfully installed dahuffman-0.4.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install dahuffman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0746ec03-3e4b-4d9e-905b-0f7cb515a83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a30d222-11d9-43fe-a5fa-78e00d2d8acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dahuffman\n",
    "import pickle\n",
    "import joblib\n",
    "import base64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8c60cb-9efd-4710-8c57-5763461a4fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dahuffman\n",
    "from tensorflow import keras\n",
    "\n",
    "class CompressibleNN(keras.Model):\n",
    "    def __init__(self, net_model):\n",
    "        super(CompressibleNN, self).__init__()\n",
    "        self.net_model = net_model\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return self.net_model(inputs)\n",
    "    \n",
    "    def compressNN(self):\n",
    "        # Get the original weights\n",
    "        weights = self.net_model.get_weights()\n",
    "\n",
    "        # Convert the weights to a flat array\n",
    "        flattened_weights = np.concatenate([w.flatten() for w in weights])\n",
    "\n",
    "        # Compress the weights using Huffman encoding\n",
    "        codec = dahuffman.HuffmanCodec.from_data(flattened_weights)\n",
    "        compressed_weights = codec.encode(flattened_weights)\n",
    "\n",
    "        # Convert the compressed weights to bytes\n",
    "        compressed_bytes = compressed_weights\n",
    "\n",
    "        return compressed_bytes\n",
    "\n",
    "    def decompressNN(self, compressed_weights):\n",
    "        # Instantiate the Huffman codec\n",
    "        codec = dahuffman.HuffmanCodec.from_binary(compressed_weights)\n",
    "\n",
    "        # Decode the compressed weights using Huffman decoding\n",
    "        deserialized_weights = codec.decode(compressed_weights)\n",
    "        \n",
    "        \n",
    "        print(deserialized_weights)\n",
    "#         # Reshape the weights to match the original shapes\n",
    "#         shapes = [w.shape for w in self.net_model.get_weights()]\n",
    "#         reshaped_weights = []\n",
    "#         idx = 0\n",
    "#         for shape in shapes:\n",
    "#             weight_size = np.prod(shape)\n",
    "#             weight_array = np.array(deserialized_weights[idx:idx+weight_size])\n",
    "#             if weight_array.size == 1 and weight_size > 1:\n",
    "#                 # Broadcast the single value to match the target shape\n",
    "#                 weight_array = np.full(shape, np.squeeze(weight_array))\n",
    "#             elif weight_array.size != weight_size:\n",
    "#                 raise ValueError(f\"Cannot reshape array of size {weight_array.size} into shape {shape}\")\n",
    "#             reshaped_weight = np.reshape(weight_array, shape)\n",
    "#             reshaped_weights.append(reshaped_weight)\n",
    "#             idx += weight_size\n",
    "        \n",
    "#         print(type(reshaped_weights))\n",
    "        \n",
    "        # Set the decompressed weights to the network\n",
    "#         self.net_model.set_weights(reshaped_weights)\n",
    "\n",
    "        return self.net_model\n",
    "\n",
    "\n",
    "    def compare_weights(self, original_weights, decompressed_weights):\n",
    "        differences = []\n",
    "        for orig, decomp in zip(original_weights, decompressed_weights):\n",
    "            diff = np.abs(orig - decomp)\n",
    "            differences.append(diff)\n",
    "        return differences\n",
    "\n",
    "# Instantiate the CompressibleNN class and create the original model\n",
    "model = get_model()  # Replace with your own model creation code\n",
    "compNN = CompressibleNN(model)\n",
    "\n",
    "# Generate some random data for testing\n",
    "random_state = np.random.RandomState(42)  # Set the seed value to 42\n",
    "inputs = np.random.rand(10, 28, 28, 1)\n",
    "\n",
    "# Get the original weights\n",
    "original_weights = model.get_weights()\n",
    "\n",
    "# Compress the weights\n",
    "compressed_weights = compNN.compressNN()\n",
    "\n",
    "# Decompress the weights\n",
    "decompressed_model = compNN.decompressNN(compressed_weights)\n",
    "decompressed_weights = decompressed_model.get_weights()\n",
    "\n",
    "# Compare the original weights with the decompressed weights\n",
    "differences = compNN.compare_weights(original_weights, decompressed_weights)\n",
    "\n",
    "# Print the maximum difference for each weight matrix\n",
    "for i, diff in enumerate(differences):\n",
    "    print(f\"Weight matrix {i}: Max difference = {np.max(diff)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8e4f720-320d-4dda-84e8-3435581c6387",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3522140960.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    workedfrom tensorflow.keras.layers import Dense, Input, Flatten\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "workedfrom tensorflow.keras.layers import Dense, Input, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import dahuffman\n",
    "\n",
    "kernel_initializer = 'he_normal'\n",
    "activation = \"relu\"\n",
    "\n",
    "\n",
    "def get_model(chs=128):\n",
    "    shape = (28, 28, 1)\n",
    "\n",
    "    inputs = Input(shape)\n",
    "    layer = Flatten()(inputs)\n",
    "    layer = Dense(units=chs * 2, activation=activation, kernel_initializer=kernel_initializer)(layer)\n",
    "    layer = Dense(units=chs, activation=activation, kernel_initializer=kernel_initializer)(layer)\n",
    "    output = Dense(10, activation='linear', use_bias=True, kernel_initializer=kernel_initializer)(layer)\n",
    "\n",
    "    model = Model(inputs, output)\n",
    "    return model\n",
    "\n",
    "class CompressibleNN(keras.Model):\n",
    "    def __init__(self, net_model):\n",
    "        super(CompressibleNN, self).__init__()\n",
    "        self.net_model = net_model\n",
    "        self.codec = []\n",
    "\n",
    "    def compressNN(self, inputs):\n",
    "        # Reshape the weights\n",
    "        reshaped_weights = self.reshape_weights(inputs)\n",
    "        print(len(reshaped_weights))\n",
    "\n",
    "        # Compress the weights using Huffman coding\n",
    "        compressed_weights = []\n",
    "        for i, weight_tensor in enumerate(reshaped_weights):\n",
    "            weight_flattened = weight_tensor.flatten()\n",
    "            weight_bytes = weight_flattened.tobytes()  # Convert to byte array\n",
    "            encoder = dahuffman.HuffmanCodec.from_data(weight_bytes)\n",
    "            self.codec.append(encoder) \n",
    "            compressed_data = encoder.encode(weight_bytes)  # Use encode() with byte array\n",
    "            compressed_weights.append(compressed_data)\n",
    "            print(\"Weight tensor shape:\", weight_tensor.shape)\n",
    "            print(\"Weight tensor size:\", weight_tensor.size)\n",
    "            print(\"Weight flattened size:\", weight_flattened.size)\n",
    "            print(\"Compressed data size:\", len(compressed_data))\n",
    "            \n",
    "        return compressed_weights\n",
    "\n",
    "\n",
    "    def decompressNN(self, compressed_weights):\n",
    "        # Decompress the weights using Huffman coding\n",
    "        decompressed_weights = []\n",
    "        for i, compressed_data in enumerate(compressed_weights):\n",
    "#             decoder = dahuffman.HuffmanCodec.from_data(compressed_data)\n",
    "            decoder = self.codec[i]\n",
    "            decompressed_data = decoder.decode(compressed_data)  # Use decode() directly\n",
    "            \n",
    "            print(\"Decompressed data size:\", len(decompressed_data))\n",
    "            weight_shape = self.net_model.get_weights()[0].shape  # Assuming all weight tensors have the same shape\n",
    "            decompressed_size = np.prod(weight_shape) * np.dtype(np.float32).itemsize\n",
    "\n",
    "            print(\"Expected decompressed size:\", decompressed_size)\n",
    "            print(\"Actual decompressed size:\", len(decompressed_data))\n",
    "\n",
    "            if len(decompressed_data) < decompressed_size:\n",
    "                decompressed_data += b'\\x00' * (decompressed_size - len(decompressed_data))\n",
    "\n",
    "            elif len(decompressed_data) > decompressed_size:\n",
    "                decompressed_data = decompressed_data[:decompressed_size]\n",
    "\n",
    "            decompressed_array = np.frombuffer(bytes(decompressed_data), dtype=np.float32)\n",
    "            decompressed_weights.append(decompressed_array.reshape(weight_shape))\n",
    "\n",
    "        return decompressed_weights\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.net_model(inputs)\n",
    "\n",
    "    def reshape_weights(self, inputs):\n",
    "        reshaped_weights = []\n",
    "        current_index = 0\n",
    "        for weight_tensor in self.net_model.get_weights():\n",
    "            weight_shape = weight_tensor.shape\n",
    "            weight_size = np.prod(weight_shape)\n",
    "            # Reshape the weight tensor based on the size of the input array\n",
    "            reshaped_weights.append(weight_tensor.reshape(weight_shape))\n",
    "            current_index += weight_size\n",
    "        return reshaped_weights\n",
    "    \n",
    "    def compare_weights(self, original_weights, decompressed_weights):\n",
    "        differences = []\n",
    "        for orig, decomp in zip(original_weights, decompressed_weights):\n",
    "            orig_shape = orig.shape\n",
    "            decomp_shape = decomp.shape\n",
    "            decomp_reshaped = decomp.reshape(orig_shape) if orig_shape == decomp_shape else decomp\n",
    "            diff = np.abs(orig - decomp_reshaped)\n",
    "            differences.append(diff)\n",
    "        return differences\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "model = get_model()\n",
    "compNN = CompressibleNN(model)\n",
    "\n",
    "# Generate some random data for testing\n",
    "random_state = np.random.RandomState(42)  # Set the seed value to 42\n",
    "inputs = random_state.rand(28, 28, 1)  # Generate random input data\n",
    "\n",
    "# Compress the weights\n",
    "compressed_weights = compNN.compressNN(inputs)\n",
    "\n",
    "# Decompress the weights\n",
    "decompressed_weights = compNN.decompressNN(compressed_weights)\n",
    "\n",
    "# print(compressed_weights)\n",
    "# print(decompressed_weights)\n",
    "\n",
    "# Compare the original weights with the decompressed weights\n",
    "differences = compNN.compare_weights(model.get_weights(), decompressed_weights)\n",
    "print(\"Differences between original and decompressed weights:\")\n",
    "for i, diff in enumerate(differences):\n",
    "    print(f\"Layer {i+1}: Max Difference = {np.max(diff)}, Mean Difference = {np.mean(diff)}\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bac1626-10ac-471d-b6af-9ce965992393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31ec596-4d8e-4a42-ab89-e4f969dfc303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "class CompressibleNN(keras.Model):\n",
    "    def __init__(self, net_model):\n",
    "        super(CompressibleNN, self).__init__()\n",
    "        self.net_model = net_model\n",
    "        self.compressor = dahuffman.HuffmanCodec.from_data([])  # Initialize an empty compressor\n",
    "        \n",
    "    def compressNN(self, inputs):\n",
    "        # Get the current weights of the network\n",
    "        weights = self.net_model.get_weights()\n",
    "\n",
    "        # Serialize the weights using joblib\n",
    "        with open(\"weights.pkl\", \"wb\") as file:\n",
    "            joblib.dump(weights, file)\n",
    "\n",
    "        # Read the serialized weights\n",
    "        with open(\"weights.pkl\", \"rb\") as file:\n",
    "            serialized_weights = file.read()\n",
    "\n",
    "        # Compress the serialized weights using Huffman encoding\n",
    "        encoder = dahuffman.HuffmanCodec.from_data(serialized_weights)\n",
    "        compressed_weights = encoder.encode(serialized_weights)\n",
    "\n",
    "        return compressed_weights\n",
    "\n",
    "    def decompressNN(self, compressed_weights):\n",
    "        # Decode the compressed weights using Huffman decoding\n",
    "        decoder = dahuffman.HuffmanCodec.from_dict(compressed_weights)\n",
    "        serialized_weights = decoder.decode(compressed_weights)\n",
    "\n",
    "        # Deserialize the weights using joblib\n",
    "        with open(\"decompressed_weights.pkl\", \"wb\") as file:\n",
    "            file.write(serialized_weights)\n",
    "\n",
    "        # Load the deserialized weights\n",
    "        with open(\"decompressed_weights.pkl\", \"rb\") as file:\n",
    "            weights = joblib.load(file)\n",
    "\n",
    "        # Set the decompressed weights to the network\n",
    "        self.net_model.set_weights(weights)\n",
    "\n",
    "        return self.net_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfd8f131-dfda-4311-835d-9bb192cbe580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight tensor shape: (784, 256)\n",
      "Weight tensor size: 200704\n",
      "Weight flattened size: 200704\n",
      "Compressed data size: 739829\n",
      "Weight tensor shape: (256,)\n",
      "Weight tensor size: 256\n",
      "Weight flattened size: 256\n",
      "Compressed data size: 128\n",
      "Weight tensor shape: (256, 128)\n",
      "Weight tensor size: 32768\n",
      "Weight flattened size: 32768\n",
      "Compressed data size: 121256\n",
      "Weight tensor shape: (128,)\n",
      "Weight tensor size: 128\n",
      "Weight flattened size: 128\n",
      "Compressed data size: 64\n",
      "Weight tensor shape: (128, 10)\n",
      "Weight tensor size: 1280\n",
      "Weight flattened size: 1280\n",
      "Compressed data size: 4712\n",
      "Weight tensor shape: (10,)\n",
      "Weight tensor size: 10\n",
      "Weight flattened size: 10\n",
      "Compressed data size: 5\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Input, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import dahuffman\n",
    "import pickle\n",
    "from Compressible_Huffman import CompressibleNN\n",
    "\n",
    "kernel_initializer = 'he_normal'\n",
    "activation = \"relu\"\n",
    "\n",
    "\n",
    "def get_model(chs=128):\n",
    "    shape = (28, 28, 1)\n",
    "\n",
    "    inputs = Input(shape)\n",
    "    layer = Flatten()(inputs)\n",
    "    layer = Dense(units=chs * 2, activation=activation, kernel_initializer=kernel_initializer)(layer)\n",
    "    layer = Dense(units=chs, activation=activation, kernel_initializer=kernel_initializer)(layer)\n",
    "    output = Dense(10, activation='linear', use_bias=True, kernel_initializer=kernel_initializer)(layer)\n",
    "\n",
    "    model = Model(inputs, output)\n",
    "    return model\n",
    "    \n",
    "\n",
    "model = get_model()\n",
    "\n",
    "with open('original_model_weights.pkl', 'wb') as file:\n",
    "    pickle.dump(model.get_weights(), file)\n",
    "compNN = CompressibleNN(model)\n",
    "\n",
    "\n",
    "# Generate some random data for testing\n",
    "random_state = np.random.RandomState(42)  # Set the seed value to 42\n",
    "inputs = random_state.rand(28, 28, 1)  # Generate random input data\n",
    "\n",
    "# Compress the weights\n",
    "compressed_weights = compNN.compressNN(inputs)\n",
    "\n",
    "# Decompress the weights\n",
    "# decompressed_weights = compNN.decompressNN(compressed_weights)\n",
    "\n",
    "# print(compressed_weights)\n",
    "# print(decompressed_weights)\n",
    "\n",
    "# Compare the original weights with the decompressed weights\n",
    "# differences = compNN.compare_weights(model.get_weights(), decompressed_weights)\n",
    "# print(\"Differences between original and decompressed weights:\")\n",
    "# for i, diff in enumerate(differences):\n",
    "#     print(f\"Layer {i+1}: Max Difference = {np.max(diff)}, Mean Difference = {np.mean(diff)}\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61cbf838-076d-49c8-9ae6-e37a978e3cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decompressed data size: 802816\n",
      "Expected decompressed size: 802816\n",
      "Actual decompressed size: 802816\n",
      "Decompressed data size: 1024\n",
      "Expected decompressed size: 1024\n",
      "Actual decompressed size: 1024\n",
      "Decompressed data size: 131072\n",
      "Expected decompressed size: 131072\n",
      "Actual decompressed size: 131072\n",
      "Decompressed data size: 512\n",
      "Expected decompressed size: 512\n",
      "Actual decompressed size: 512\n",
      "Decompressed data size: 5120\n",
      "Expected decompressed size: 5120\n",
      "Actual decompressed size: 5120\n",
      "Decompressed data size: 40\n",
      "Expected decompressed size: 40\n",
      "Actual decompressed size: 40\n",
      "Differences between original and decompressed weights:\n",
      "Layer 1: Max Difference = 0.0, Mean Difference = 0.0\n",
      "Layer 2: Max Difference = 0.0, Mean Difference = 0.0\n",
      "Layer 3: Max Difference = 0.0, Mean Difference = 0.0\n",
      "Layer 4: Max Difference = 0.0, Mean Difference = 0.0\n",
      "Layer 5: Max Difference = 0.0, Mean Difference = 0.0\n",
      "Layer 6: Max Difference = 0.0, Mean Difference = 0.0\n"
     ]
    }
   ],
   "source": [
    "decompressed_weights = compNN.decompressNN(compressed_weights)\n",
    "\n",
    "# print(compressed_weights)\n",
    "# print(decompressed_weights)\n",
    "\n",
    "# Compare the original weights with the decompressed weights\n",
    "differences = compNN.compare_weights(model.get_weights(), decompressed_weights)\n",
    "\n",
    "print(\"Differences between original and decompressed weights:\")\n",
    "for i, diff in enumerate(differences):\n",
    "    print(f\"Layer {i+1}: Max Difference = {np.max(diff)}, Mean Difference = {np.mean(diff)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88df797-4c3b-4e9c-92c0-de727d2fd655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.0387225 , -0.08137392, -0.02914773, ..., -0.00919168,\n",
      "         0.02946452,  0.04639422],\n",
      "       [-0.05397551, -0.00483679, -0.04930591, ...,  0.07424383,\n",
      "         0.0271853 , -0.01563342],\n",
      "       [-0.00271451, -0.0424049 , -0.00124385, ...,  0.00815904,\n",
      "        -0.0186043 , -0.02493658],\n",
      "       ...,\n",
      "       [ 0.06878325,  0.03475486,  0.06290235, ...,  0.04817623,\n",
      "        -0.08055975,  0.05437632],\n",
      "       [-0.07352863,  0.01516827,  0.10736844, ..., -0.01694267,\n",
      "         0.06866149,  0.00142415],\n",
      "       [-0.03066814,  0.03886299,  0.03796422, ...,  0.08156086,\n",
      "         0.02407725, -0.06137323]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0.], dtype=float32), array([[-0.05473082, -0.08747619, -0.09267823, ...,  0.03438703,\n",
      "         0.05442026, -0.14908285],\n",
      "       [ 0.12407006, -0.06414388,  0.0274786 , ...,  0.08441756,\n",
      "         0.01513519, -0.10123061],\n",
      "       [ 0.14844115, -0.1502237 ,  0.09429957, ..., -0.16309366,\n",
      "        -0.07074652, -0.000291  ],\n",
      "       ...,\n",
      "       [ 0.02375787,  0.09773865,  0.07689589, ..., -0.03664695,\n",
      "         0.16372688,  0.12212976],\n",
      "       [-0.03579683,  0.0100025 , -0.02489873, ...,  0.09990413,\n",
      "        -0.1348389 , -0.01136187],\n",
      "       [ 0.15497929, -0.12766743,  0.03431471, ...,  0.03162755,\n",
      "         0.04003979,  0.00781596]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[-0.21259066, -0.01089567, -0.02497171, ...,  0.02639445,\n",
      "         0.06686264,  0.21426989],\n",
      "       [ 0.04711202,  0.05810664, -0.05675338, ...,  0.16335574,\n",
      "        -0.11026386,  0.03047448],\n",
      "       [-0.06761184, -0.17188099,  0.05826836, ..., -0.03980751,\n",
      "         0.16168733,  0.0682351 ],\n",
      "       ...,\n",
      "       [-0.00555486,  0.20761333,  0.00415648, ..., -0.02198105,\n",
      "         0.05630907, -0.02674305],\n",
      "       [ 0.15399532, -0.1474021 , -0.13314971, ...,  0.09355479,\n",
      "        -0.06023131,  0.0226844 ],\n",
      "       [ 0.11596036,  0.14716065, -0.0693124 , ..., -0.01054673,\n",
      "         0.07708199, -0.15401456]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c472479-1619-49e5-a657-c71d91bfc7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.0387225 , -0.08137392, -0.02914773, ..., -0.00919168,\n",
      "         0.02946452,  0.04639422],\n",
      "       [-0.05397551, -0.00483679, -0.04930591, ...,  0.07424383,\n",
      "         0.0271853 , -0.01563342],\n",
      "       [-0.00271451, -0.0424049 , -0.00124385, ...,  0.00815904,\n",
      "        -0.0186043 , -0.02493658],\n",
      "       ...,\n",
      "       [ 0.06878325,  0.03475486,  0.06290235, ...,  0.04817623,\n",
      "        -0.08055975,  0.05437632],\n",
      "       [-0.07352863,  0.01516827,  0.10736844, ..., -0.01694267,\n",
      "         0.06866149,  0.00142415],\n",
      "       [-0.03066814,  0.03886299,  0.03796422, ...,  0.08156086,\n",
      "         0.02407725, -0.06137323]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0.], dtype=float32), array([[-0.05473082, -0.08747619, -0.09267823, ...,  0.03438703,\n",
      "         0.05442026, -0.14908285],\n",
      "       [ 0.12407006, -0.06414388,  0.0274786 , ...,  0.08441756,\n",
      "         0.01513519, -0.10123061],\n",
      "       [ 0.14844115, -0.1502237 ,  0.09429957, ..., -0.16309366,\n",
      "        -0.07074652, -0.000291  ],\n",
      "       ...,\n",
      "       [ 0.02375787,  0.09773865,  0.07689589, ..., -0.03664695,\n",
      "         0.16372688,  0.12212976],\n",
      "       [-0.03579683,  0.0100025 , -0.02489873, ...,  0.09990413,\n",
      "        -0.1348389 , -0.01136187],\n",
      "       [ 0.15497929, -0.12766743,  0.03431471, ...,  0.03162755,\n",
      "         0.04003979,  0.00781596]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[-0.21259066, -0.01089567, -0.02497171, ...,  0.02639445,\n",
      "         0.06686264,  0.21426989],\n",
      "       [ 0.04711202,  0.05810664, -0.05675338, ...,  0.16335574,\n",
      "        -0.11026386,  0.03047448],\n",
      "       [-0.06761184, -0.17188099,  0.05826836, ..., -0.03980751,\n",
      "         0.16168733,  0.0682351 ],\n",
      "       ...,\n",
      "       [-0.00555486,  0.20761333,  0.00415648, ..., -0.02198105,\n",
      "         0.05630907, -0.02674305],\n",
      "       [ 0.15399532, -0.1474021 , -0.13314971, ...,  0.09355479,\n",
      "        -0.06023131,  0.0226844 ],\n",
      "       [ 0.11596036,  0.14716065, -0.0693124 , ..., -0.01054673,\n",
      "         0.07708199, -0.15401456]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(decompressed_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d934cf3-e0ff-496c-aeba-7e1ff0afe2d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "51",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m compressed_weights \u001b[38;5;241m=\u001b[39m compNN\u001b[38;5;241m.\u001b[39mcompressNN(inputs)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Decompress the weights\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m decompressed_weights \u001b[38;5;241m=\u001b[39m \u001b[43mcompNN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecompressNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompressed_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Compare the original weights with the decompressed weights\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# differences = compNN.compare_weights(original_weights, decompressed_weights)\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# # Print the maximum difference for each weight matrix\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[88], line 38\u001b[0m, in \u001b[0;36mCompressibleNN.decompressNN\u001b[0;34m(self, compressed_weights)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Load the deserialized weights\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecompressed_weights.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m---> 38\u001b[0m     weights \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Set the decompressed weights to the network\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet_model\u001b[38;5;241m.\u001b[39mset_weights(weights)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/numpy_pickle.py:648\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    646\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fobj, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    647\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(fobj, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[0;32m--> 648\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43m_unpickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/numpy_pickle.py:577\u001b[0m, in \u001b[0;36m_unpickle\u001b[0;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[1;32m    575\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 577\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m unpickler\u001b[38;5;241m.\u001b[39mcompat_mode:\n\u001b[1;32m    579\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has been generated with a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoblib version less than 0.10. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease regenerate this pickle file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m                       \u001b[38;5;241m%\u001b[39m filename,\n\u001b[1;32m    583\u001b[0m                       \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/pickle.py:1213\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1211\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[0;32m-> 1213\u001b[0m         \u001b[43mdispatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n\u001b[1;32m   1215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stopinst\u001b[38;5;241m.\u001b[39mvalue\n",
      "\u001b[0;31mKeyError\u001b[0m: 51"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "class CompressibleNN(keras.Model):\n",
    "    def __init__(self, net_model):\n",
    "        super(CompressibleNN, self).__init__()\n",
    "        self.net_model = net_model\n",
    "        self.compressor = dahuffman.HuffmanCodec.from_data([])  # Initialize an empty compressor\n",
    "        \n",
    "    def compressNN(self, inputs):\n",
    "        # Get the current weights of the network\n",
    "        weights = self.net_model.get_weights()\n",
    "\n",
    "        # Serialize the weights using joblib\n",
    "        with open(\"weights.pkl\", \"wb\") as file:\n",
    "            joblib.dump(weights, file)\n",
    "\n",
    "        # Read the serialized weights\n",
    "        with open(\"weights.pkl\", \"rb\") as file:\n",
    "            serialized_weights = file.read()\n",
    "\n",
    "        # Compress the serialized weights using Huffman encoding\n",
    "        encoder = dahuffman.HuffmanCodec.from_data(serialized_weights)\n",
    "        compressed_weights = encoder.encode(serialized_weights)\n",
    "\n",
    "        return compressed_weights\n",
    "\n",
    "    def decompressNN(self, compressed_weights):\n",
    "        # Decode the compressed weights using Huffman decoding\n",
    "        decoder = dahuffman.HuffmanCodec.from_data(compressed_weights)\n",
    "        serialized_weights = decoder.decode(compressed_weights)\n",
    "\n",
    "        # Deserialize the weights using joblib\n",
    "        with open(\"decompressed_weights.pkl\", \"wb\") as file:\n",
    "            file.write(serialized_weights)\n",
    "\n",
    "        # Load the deserialized weights\n",
    "        with open(\"decompressed_weights.pkl\", \"rb\") as file:\n",
    "            weights = joblib.load(file)\n",
    "\n",
    "        # Set the decompressed weights to the network\n",
    "        self.net_model.set_weights(weights)\n",
    "\n",
    "        return self.net_model\n",
    "\n",
    "\n",
    "# Instantiate the CompressibleNN class and create the original model\n",
    "model = get_model()  # Replace with your own model creation code\n",
    "\n",
    "# Save the original model's architecture\n",
    "original_architecture = model.to_json()\n",
    "\n",
    "compNN = CompressibleNN(model)\n",
    "\n",
    "# Generate some random data for testing\n",
    "random_state = np.random.RandomState(42)  # Set the seed value to 42\n",
    "inputs = np.random.rand(10, 28, 28, 1)\n",
    "\n",
    "# Compress the weights\n",
    "compressed_weights = compNN.compressNN(inputs)\n",
    "\n",
    "# Decompress the weights\n",
    "decompressed_weights = compNN.decompressNN(compressed_weights)\n",
    "\n",
    "# Compare the original weights with the decompressed weights\n",
    "# differences = compNN.compare_weights(original_weights, decompressed_weights)\n",
    "\n",
    "# # Print the maximum difference for each weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f16e96c-2803-49d9-8860-5b3c47c99b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
