{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf318b02-028e-4235-9f03-048a7d41e433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# set module auto-reloaded \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01627cc9-b85e-4bd4-a4de-74e37afc4e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Input, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import dahuffman\n",
    "import pickle\n",
    "from Compressible_Huffman import Huffman\n",
    "from getModel import get_simplemodel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72c02018-fbc7-4b1a-8988-97c9aa95f7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CompressibleNN instance\n",
    "import os\n",
    "\n",
    "compNN_list = []\n",
    "cnt = 0\n",
    "directory = 'results'\n",
    "decompressed_weights_list = []\n",
    "\n",
    "while True:\n",
    "    # Check if the file exists\n",
    "    filename = f'{directory}/compressed_nn_{cnt}.pkl'\n",
    "    if not os.path.exists(filename):\n",
    "        break\n",
    "\n",
    "    # Load the compressed model from the file\n",
    "    with open(filename, 'rb') as compressed_nn:\n",
    "        net_model = pickle.load(compressed_nn)\n",
    "        codec = pickle.load(compressed_nn)\n",
    "        compressed_model_weights = pickle.load(compressed_nn)\n",
    "\n",
    "    # Create a new instance of CompressibleNN\n",
    "    compNN = Huffman(net_model)\n",
    "    compNN.codec = codec\n",
    "    compNN_list.append(compNN)\n",
    "    decompressed_weights = compNN.decompressNN(compressed_model_weights)\n",
    "    decompressed_weights_list.append(decompressed_weights)\n",
    "    \n",
    "    # Increment the counter\n",
    "    cnt += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc8fc49-f1eb-4d8a-a1d3-881c8c25ef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional\n",
    "# Compare the original weights with the decompressed weights\n",
    "original_model_weights_list = []\n",
    "for cnt, compNN in enumerate(compNN_list):\n",
    "    # Check if the file exists\n",
    "    filename = f'{directory}/original_model{cnt}_weights.pkl'\n",
    "    if not os.path.exists(filename):\n",
    "        break\n",
    "    with open(filename, 'rb') as file:\n",
    "        original_model_weights = pickle.load(file)\n",
    "    original_model_weights_list.append(original_model_weights)\n",
    "        \n",
    "    differences = compNN.compare_weights(original_model_weights_list[cnt], decompressed_weights_list[cnt])\n",
    "\n",
    "    print(f\"Differences between original and decompressed weights in model{cnt}:\")\n",
    "    for i, diff in enumerate(differences):\n",
    "        print(f\"Layer {i+1}: Max Difference = {np.max(diff)}, Mean Difference = {np.mean(diff)}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d05208ae-d77c-4b8a-95ed-908b2b11f170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set decompressed weights\n",
    "for compNN, decompressed_weights in zip(compNN_list, decompressed_weights_list):\n",
    "    compNN.set_weights(decompressed_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c09e2dc-334b-4f87-a346-5d93d4a8ff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "image_size = x_train.shape[1] # 28\n",
    "input_size = image_size * image_size\n",
    "\n",
    "# Preprocess the data (reshape and normalize)\n",
    "x_train = np.reshape(x_train, [-1, image_size, image_size, 1])  # Keep the shape (None, 28, 28, 1)\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = np.reshape(x_test, [-1, image_size, image_size, 1])  # Keep the shape (None, 28, 28, 1)\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "results = []\n",
    "\n",
    "for cnt, compNN in enumerate(compNN_list): \n",
    "    # Make predictions using your model\n",
    "    predictions = compNN.predict(x_test)\n",
    "\n",
    "    # Calculate top-1 accuracy\n",
    "    predicted_labels = tf.argmax(predictions, axis=1)\n",
    "    top_1_accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted_labels, y_test), tf.float32)) * 100\n",
    "\n",
    "    print(f\"Model{cnt} Top-1 Accuracy: {top_1_accuracy:.3f}%\")\n",
    "    results.append(f\"Model{cnt} Top-1 Accuracy: {top_1_accuracy:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4cc93dd5-cdee-4229-b7e6-803f87e54a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the full path to the log file\n",
    "log_filename = os.path.join(directory, \"accuracy_logs.txt\")\n",
    "\n",
    "# Save the results to the file\n",
    "with open(log_filename, \"w\") as file:\n",
    "    for result_entry in results:\n",
    "        file.write(result_entry + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
